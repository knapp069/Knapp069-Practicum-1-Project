{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae1f47c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on Textual Data\n",
    "\n",
    "This notebook is designed to perform Exploratory Data Analysis (EDA) on textual data. It covers the process from initial data loading to cleaning, analyzing, and visualizing the data. Libraries used include pandas, matplotlib, seaborn, nltk, TextBlob, WordCloud, and spacy.\n",
    "\n",
    "After cleaning, the data is written to 'cleaned_section_data.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Download NLTK resources for tokenization and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3934",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "Define functions for plotting word frequencies, cleaning text data, and other repetitive tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot word frequencies\n",
    "def plot_word_frequencies(freq_dist, title, num_words=30):\n",
    "    words, counts = zip(*freq_dist.most_common(num_words))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(words), y=list(counts))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    # Frequet Terms to remove while cleaning\n",
    "    other_terms = [\n",
    "        \"copyright\",\n",
    "        \"2024\",\n",
    "        \"logrhythm inc\"\n",
    "        \"all rights reserved\",\n",
    "        \"â€¢\",\n",
    "        \"powered by\",\n",
    "        \"scroll viewport\",\n",
    "        \"&\",\n",
    "        \"atlassian confluence\",\n",
    "        \"please note these errors can depend on your browser setup\"\n",
    "    ]\n",
    "    words = [word for word in words if word not in stop_words and word not in other_terms]\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(' '.join(words)) if not token.is_stop]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c710c",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Load the textual data from a CSV file into a pandas DataFrame. This data will be used for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6447f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "section_data = pd.read_csv('section_data.csv')\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"DataFrame Information:\")\n",
    "section_data.info()\n",
    "\n",
    "# Display descriptive statistics of the DataFrame\n",
    "print(\"\\nDataFrame Description:\")\n",
    "section_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71bae47",
   "metadata": {},
   "source": [
    "### Initial Exploratory Data Analysis on Unprocessed Data\n",
    "\n",
    "Perform initial EDA on the raw data to understand its basic structure, including text length distribution, word frequency analysis, word cloud generation, and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text length distribution\n",
    "section_data['Text Length'] = section_data['content'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Text Length Distribution')\n",
    "sns.histplot(section_data['Text Length'], bins=50, kde=True)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Perform word frequency analysis\n",
    "freq_dist = FreqDist(word_tokenize(' '.join(section_data['content'])))\n",
    "plot_word_frequencies(freq_dist, 'Top Words in Raw Data')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(section_data['content']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Raw Data')\n",
    "plt.show()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "section_data['Sentiment'] = section_data['content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Sentiment Distribution')\n",
    "sns.histplot(section_data['Sentiment'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6b58d",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Clean the textual data by removing unnecessary characters, stopwords, and lemmatizing the text. Also, remove duplicates and blank entries to ensure the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text cleaning function\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "section_data['Cleaned Text'] = section_data['content'].progress_apply(clean_text)\n",
    "\n",
    "# Remove duplicates in the data\n",
    "print(\"Removing duplicates...\")\n",
    "section_data = section_data.drop_duplicates(subset=['Cleaned Text'])\n",
    "print(\"Duplicates removed.\")\n",
    "\n",
    "# Remove blank entries from the data\n",
    "print(\"Removing blank entries...\")\n",
    "section_data = section_data[section_data['Cleaned Text'].str.strip() != '']\n",
    "print(\"Blank entries removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eedb38",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis on Cleaned Text\n",
    "\n",
    "Repeat the EDA process on the cleaned text data. This includes analyzing the text length distribution, word frequency, word cloud, and sentiment for the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display updated information about the DataFrame\n",
    "print(\"DataFrame Information:\")\n",
    "section_data.info()\n",
    "\n",
    "# Display descriptive statistics of the cleaned DataFrame\n",
    "print(\"\\nDataFrame Description:\")\n",
    "section_data.describe()\n",
    "\n",
    "# Analyze text length distribution after cleaning\n",
    "section_data['Cleaned Text Length'] = section_data['Cleaned Text'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Text Length Distribution After Cleaning')\n",
    "sns.histplot(section_data['Cleaned Text Length'], bins=50, kde=True)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Perform word frequency analysis on cleaned text\n",
    "freq_dist_cleaned = FreqDist(word_tokenize(' '.join(section_data['Cleaned Text'])))\n",
    "plot_word_frequencies(freq_dist_cleaned, 'Top Words in Cleaned Data')\n",
    "\n",
    "# Generate a word cloud for cleaned text\n",
    "wordcloud_cleaned = WordCloud(width=800, height=400, background_color='white').generate(' '.join(section_data['Cleaned Text']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Cleaned Data')\n",
    "plt.show()\n",
    "\n",
    "# Perform sentiment analysis on cleaned text\n",
    "section_data['Cleaned Sentiment'] = section_data['Cleaned Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Sentiment Distribution After Cleaning')\n",
    "sns.histplot(section_data['Cleaned Sentiment'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88786e8d",
   "metadata": {},
   "source": [
    "### Saving Cleaned Data\n",
    "\n",
    "After the analysis, save the cleaned and processed data to a new CSV file for future use or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "section_data.to_csv('cleaned_section_data.csv', index=False)\n",
    "print(\"Cleaned data has been saved to 'cleaned_section_data.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
