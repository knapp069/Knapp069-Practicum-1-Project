{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f6a23a",
   "metadata": {},
   "source": [
    "# Web Scraping Script for Data Extraction\n",
    "\n",
    "This script is designed for web scraping and extracting information from a docs.logrhythm.com about the 7.15 version of the software. It includes various utility functions and a crawling mechanism to navigate and process web pages. The script uses libraries such as `requests`, `BeautifulSoup`, `urlparse`, and `pandas`. Please ensure that these libraries are installed in your Jupyter environment before running the script.\n",
    "\n",
    "The data scraped from the website is stored in a CSV file for later use in processing.  The file is named 'section_data.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed3daa",
   "metadata": {},
   "source": [
    "### Imports and set Seed for Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58969985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from langdetect import detect, DetectorFactory\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set seed for consistent language detection\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63645f44",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following cells define several utility functions that are essential for the web scraping process. These include functions to fetch page content, check compliance with `robots.txt`, extract and filter sectioned content, check if the content is in English, and extract valid links from a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2641fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to fetch page content with retry logic\n",
    "def get_page_content(url, max_retries=3, delay=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response.content\n",
    "            else:\n",
    "                retries += 1\n",
    "                time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "# Function to check if crawling a URL is allowed by the site's robots.txt\n",
    "def is_allowed_by_robots(url):\n",
    "    base_url = urljoin(url, '/')\n",
    "    if not base_url.endswith('/'):\n",
    "        base_url += '/'\n",
    "    robots_txt_url = urljoin(base_url, 'robots.txt')\n",
    "    try:\n",
    "        response = requests.get(robots_txt_url)\n",
    "        response.raise_for_status()\n",
    "        robots_txt_content = response.text\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "    rp = RobotFileParser()\n",
    "    rp.parse(robots_txt_content.splitlines())\n",
    "    return rp.can_fetch(\"*\", url)\n",
    "\n",
    "# List of keywords to exclude in content sections, to filter out irrelevant sections\n",
    "exclude_keywords = ['general', 'release notes', 'key highlights', 'maintenance', 'retrived']  \n",
    "\n",
    "# Function to parse and extract relevant sections from HTML content\n",
    "def extract_sectioned_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    sections = {}\n",
    "    current_header = None\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
    "        if element.name in ['h1', 'h2', 'h3']:\n",
    "            header_text = element.get_text(strip=True).lower()\n",
    "            if \"general\" in header_text or any(keyword.lower() in header_text for keyword in exclude_keywords):\n",
    "                current_header = None\n",
    "            else:\n",
    "                current_header = header_text\n",
    "                sections[current_header] = ''\n",
    "        elif current_header:\n",
    "            sections[current_header] += element.get_text(separator='\\n', strip=True) + '\\n'\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function to determine if the content is in English for processing\n",
    "def is_english_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meaningful_text = soup.body.get_text(separator=' ', strip=True) if soup.body else ''\n",
    "    try:\n",
    "        return detect(meaningful_text[:1000]) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to extract and validate hyperlinks from the HTML content\n",
    "def extract_links(html_content, base_url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    valid_links = []\n",
    "    for link in links:\n",
    "        if not link.startswith('http'):\n",
    "            link = urljoin(base_url, link)\n",
    "        parsed_link = urlparse(link)\n",
    "        if parsed_link.netloc == 'docs.logrhythm.com' and parsed_link.path.startswith('/lrsiem/7.15.0/'):\n",
    "            valid_links.append(link)\n",
    "    return valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3aa2e",
   "metadata": {},
   "source": [
    "### Crawling Function\n",
    "\n",
    "The `crawl` function is the core of the script. It recursively visits links, checks for `robots.txt` compliance, and processes pages within the specified domain. Extracted content is aggregated into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and concatenate content from different sections of a webpage\n",
    "def extract_information(html_content, url):\n",
    "    sectioned_content = extract_sectioned_content(html_content)\n",
    "    concatenated_content = '\\n'.join([section for section in sectioned_content.values()])\n",
    "    return concatenated_content\n",
    "\n",
    "# Initialize a list to store the scraped content\n",
    "scraped_data = []\n",
    "\n",
    "# Recursive function to crawl and process webpages\n",
    "def crawl(url, visited, delay=0.5):\n",
    "    if url in visited or not url.startswith('http'):\n",
    "        return\n",
    "    visited.add(url)\n",
    "    print(f\"Visiting and processing: {url}\")\n",
    "    time.sleep(delay)  # Rate limiting\n",
    "    if is_allowed_by_robots(url) and urlparse(url).netloc == 'docs.logrhythm.com':\n",
    "        content = get_page_content(url)\n",
    "        if content and is_english_content(content):\n",
    "            concatenated_content = extract_information(content, url)\n",
    "            scraped_data.append(concatenated_content)\n",
    "            for link in extract_links(content, url):\n",
    "                crawl(link, visited, delay)\n",
    "    else:\n",
    "        print(f\"Crawling not allowed or outside domain for: {url}\")\n",
    "\n",
    "# Start URL for Crawling\n",
    "start_url = \"https://docs.logrhythm.com/lrsiem/7.15.0/\"\n",
    "visited_urls = set()\n",
    "\n",
    "# Initiating the crawling process from the start URL\n",
    "crawl(start_url, visited_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f80d7",
   "metadata": {},
   "source": [
    "### Data Processing and Output\n",
    "\n",
    "After crawling, the script processes the scraped data, saves it to a CSV file, and displays samples from the DataFrame. The pandas settings are adjusted for better display of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7fb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any data was scraped and process it\n",
    "if scraped_data:\n",
    "    section_data = pd.DataFrame(scraped_data, columns=['content'])\n",
    "    section_data.to_csv('section_data.csv', index=False, encoding='utf-8')\n",
    "    print(\"Dataframe created and .csv file saved.\")\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n",
    "\n",
    "# Setting display options for better visualization of the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Displaying the first and last few documents in the DataFrame\n",
    "print('Examples from start of the dataframe:\\n')\n",
    "print(section_data.head(3))  \n",
    "print('\\nExamples from end of the dataframe:')\n",
    "print(section_data.tail(3))  \n",
    "\n",
    "# Resetting display options to default\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
