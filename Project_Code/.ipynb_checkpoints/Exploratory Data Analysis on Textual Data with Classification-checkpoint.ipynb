{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae1f47c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on Textual Data\n",
    "\n",
    "This notebook is designed to perform Exploratory Data Analysis (EDA) on textual data. It covers the process from initial data loading to cleaning, analyzing, and visualizing the data. The main steps include:\n",
    "\n",
    "1. **Data Loading**: Load textual data from a CSV file into a pandas DataFrame.\n",
    "2. **Initial Exploratory Data Analysis on Unprocessed Data**: Perform initial EDA on the raw data to understand its basic structure, including text length distribution, word frequency analysis, word cloud generation, and sentiment analysis.\n",
    "3. **Data Cleaning**: Clean the textual data by removing unnecessary characters, stopwords, and lemmatizing the text. Also, remove duplicates and blank entries to ensure the quality of the dataset.\n",
    "4. **Extract Category Feature from Document Text**: Define categories and corresponding keywords for classification. Assign categories to each document based on keywords.\n",
    "5. **Exploratory Data Analysis on Cleaned Text**: Repeat the EDA process on the cleaned text data. This includes analyzing the text length distribution, word frequency, word cloud, sentiment, and distribution of assigned categories from text.\n",
    "6. **Saving Cleaned Data**: Save the cleaned and processed data to a new CSV file for future use or further analysis. After cleaning, the data is written to 'cleaned_section_data_with_categories.csv.\n",
    "\n",
    "Libraries used include pandas, matplotlib, seaborn, nltk, TextBlob, WordCloud, and spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from langdetect import detect, LangDetectException\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb1670",
   "metadata": {},
   "source": [
    "### Load Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Download NLTK resources for tokenization and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac3934",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "Define functions for plotting word frequencies, cleaning text data, and other repetitive tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e336c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot word frequencies\n",
    "def plot_word_frequencies(freq_dist, title, num_words=30):\n",
    "    \"\"\"\n",
    "    Plot word frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - freq_dist: Frequency distribution of words.\n",
    "    - title: Title of the plot.\n",
    "    - num_words: Number of top words to display.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    words, counts = zip(*freq_dist.most_common(num_words))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(words), y=list(counts))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text data\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text data by removing unnecessary characters, stopwords, and lemmatizing the text.\n",
    "    Also, remove specific text from the original content.\n",
    "\n",
    "    Parameters:\n",
    "    - text: Input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - Cleaned text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the text is NaN\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "\n",
    "        # Remove specific text using regular expressions\n",
    "        specific_text = r\"Copyright © 2024 LogRhythm, Inc\\. All Rights Reserved.*?If this problem persists, please contact our support\\.\"\n",
    "        text = re.sub(specific_text, '', text, flags=re.DOTALL)\n",
    "\n",
    "        # Language detection and filter out non-English\n",
    "        if detect(text) != 'en':\n",
    "            return ''\n",
    "\n",
    "        # List of additional characters to remove\n",
    "        characters_to_remove = [\n",
    "            'â€”', 'â€™', 'â€œ', 'â€¦', 'é', 'ø', 'à', 'ç', 'ê', 'ä', 'ü', 'ñ', 'î', 'è', 'ø', 'ü', 'ê'\n",
    "        ]\n",
    "\n",
    "        # Remove each character in the list\n",
    "        for char in characters_to_remove:\n",
    "            text = text.replace(char, '')\n",
    "        \n",
    "        # Convert text to lowercase and tokenize\n",
    "        text = ''.join([char.lower() for char in text if char.isalnum() or char.isspace()])\n",
    "        words = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatized_tokens = [token.lemma_ for token in nlp(' '.join(words)) if not token.is_stop]\n",
    "\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    except LangDetectException:\n",
    "        # Handle texts that langdetect can't process\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ddce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove frequent Trigram Phrases\n",
    "def remove_frequent_trigrams(df, column_name, threshold=300):\n",
    "    \"\"\"\n",
    "    Remove frequent trigrams from the text.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the text data.\n",
    "    - column_name: Name of the column containing the text.\n",
    "    - threshold: Threshold frequency for trigrams.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with frequent trigrams removed.\n",
    "    \"\"\"\n",
    "    # Tokenize the text and create trigrams\n",
    "    all_trigrams = [' '.join(gram) for text in df[column_name] for gram in trigrams(text.split())]\n",
    "\n",
    "    # Count the frequency of each trigram\n",
    "    trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "    # Identify trigrams that occur more than the threshold\n",
    "    frequent_trigrams = {gram for gram, freq in trigram_freq.items() if freq > threshold}\n",
    "\n",
    "    # Function to remove frequent trigrams from a text\n",
    "    def remove_trigrams(text):\n",
    "        return ' '.join([' '.join(gram) for gram in trigrams(text.split()) if ' '.join(gram) not in frequent_trigrams])\n",
    "\n",
    "    # Apply the function to the DataFrame with a progress bar\n",
    "    tqdm.pandas(desc=\"Removing Frequent Trigrams from cleaned data\")\n",
    "    df[column_name] = df[column_name].progress_apply(remove_trigrams)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to describe dataframe attributes\n",
    "def describe_dataframe(df):\n",
    "    \"\"\"\n",
    "    Describe DataFrame attributes including information, descriptive statistics, data types,\n",
    "    missing values, and unique values for each column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be described.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"DataFrame Information:\")\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\nDataFrame Descriptive Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing Values in Each Column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "    print(\"\\nUnique Values in Each Column:\")\n",
    "    for col in df.columns:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f'Column {col} has {unique_count} unique values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0943da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify unique charaters\n",
    "def unique_characters_count(text_column):\n",
    "    \"\"\"\n",
    "    Count unique characters in a text column.\n",
    "\n",
    "    Parameters:\n",
    "    - text_column: Column containing text data.\n",
    "\n",
    "    Returns:\n",
    "    - List of unique characters sorted by counts.\n",
    "    \"\"\"\n",
    "    # Concatenate all text into a single string\n",
    "    all_text = ''.join(text_column)\n",
    "\n",
    "    # Create a dictionary to count occurrences of each character\n",
    "    character_counts = {}\n",
    "    for char in all_text:\n",
    "        if char in character_counts:\n",
    "            character_counts[char] += 1\n",
    "        else:\n",
    "            character_counts[char] = 1\n",
    "\n",
    "    # Sort the dictionary by counts in descending order\n",
    "    sorted_character_counts = sorted(character_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_character_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573806d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to count frequent words and phrases in text\n",
    "def find_frequent_ngrams(df, column_name, num_terms=10, ngram_size=1):\n",
    "    \"\"\"\n",
    "    Find frequent n-grams in the text.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the text data.\n",
    "    - column_name: Name of the column containing the text.\n",
    "    - num_terms: Number of top terms to display.\n",
    "    - ngram_size: Size of n-grams (1 for unigrams, 2 for bigrams, 3 for trigrams).\n",
    "\n",
    "    Returns:\n",
    "    - List of most common n-grams.\n",
    "    \"\"\"\n",
    "    ngrams_list = []\n",
    "\n",
    "    # Iterate through each row to generate n-grams\n",
    "    for text in df[column_name]:\n",
    "        tokens = text.split()\n",
    "        \n",
    "        if ngram_size == 1:\n",
    "            ngrams = tokens\n",
    "        elif ngram_size == 2:\n",
    "            ngrams = [' '.join(gram) for gram in bigrams(tokens)]\n",
    "        elif ngram_size == 3:\n",
    "            ngrams = [' '.join(gram) for gram in trigrams(tokens)]\n",
    "        else:\n",
    "            raise ValueError(\"ngram_size must be 1, 2, or 3\")\n",
    "\n",
    "        ngrams_list.extend(ngrams)\n",
    "\n",
    "    # Count the frequency of each n-gram\n",
    "    freq_dist = Counter(ngrams_list)\n",
    "\n",
    "    # Get the most common n-grams\n",
    "    common_ngrams = freq_dist.most_common(num_terms)\n",
    "\n",
    "    return common_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign a category to a text based on keywords\n",
    "def assign_category(text, categories):\n",
    "    \"\"\"\n",
    "    Assign a category to a text based on keywords.\n",
    "\n",
    "    Parameters:\n",
    "    - text: Input text to be categorized.\n",
    "    - categories: Dictionary containing category names as keys and corresponding keywords as values.\n",
    "\n",
    "    Returns:\n",
    "    - Assigned category.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    category_scores = {category: 0 for category in categories.keys()}\n",
    "    for category, keywords in categories.items():\n",
    "        category_scores[category] += sum(text.count(keyword) for keyword in keywords)\n",
    "    assigned_category = max(category_scores, key=category_scores.get)\n",
    "    return 'Other' if category_scores[assigned_category] == 0 else assigned_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158619a",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Load the textual data from a CSV file into a pandas DataFrame. This data will be used for the subsequent analysis.  This also copies the original 'section_data.csv' to 'original_section_data.csv' before it overwrites the 'content' column of the 'section_data.csv' with specific text removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984df34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "section_data = pd.read_csv('section_data.csv', encoding='utf-8')\n",
    "\n",
    "# Copy the DataFrame to create a new one for preservation\n",
    "original_section_data = section_data.copy()\n",
    "\n",
    "# Save the original DataFrame to a new CSV file\n",
    "original_section_data.to_csv('original_section_data.csv', index=False)\n",
    "\n",
    "# Function to remove specific text from the 'content' column\n",
    "def remove_specific_text(text):\n",
    "    specific_text = r\"Copyright © 2024 LogRhythm, Inc\\. All Rights Reserved.*?If this problem persists, please contact our support\\.\"\n",
    "    return re.sub(specific_text, '', text, flags=re.DOTALL)\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "section_data['content'] = section_data['content'].apply(remove_specific_text)\n",
    "\n",
    "# Save the updated DataFrame to the same CSV file, overwriting the original\n",
    "section_data.to_csv('section_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c1a5a",
   "metadata": {},
   "source": [
    "# Initial Exploratory Data Analysis on Unprocessed Data\n",
    "\n",
    "Perform initial EDA on the raw data to understand its basic structure, including text length distribution, word frequency analysis, word cloud generation, and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the DataFrame before text cleaning\n",
    "describe_dataframe(section_data)\n",
    "\n",
    "# Check for duplicate text\n",
    "duplicate_text = section_data[section_data.duplicated(['content'])]\n",
    "\n",
    "# Print the duplicates\n",
    "print(\"Duplicate Text Entries:\")\n",
    "print(duplicate_text)\n",
    "\n",
    "# Check for missing data in each column\n",
    "missing_data = section_data.isnull().sum()\n",
    "\n",
    "# Print columns with missing data\n",
    "print(\"Columns with Missing Data:\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# Analyze text length distribution\n",
    "section_data['Text Length'] = section_data['content'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Text Length Distribution')\n",
    "sns.histplot(section_data['Text Length'], bins=50, kde=True)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Perform word frequency analysis\n",
    "freq_dist = FreqDist(word_tokenize(' '.join(section_data['content'])))\n",
    "plot_word_frequencies(freq_dist, 'Top Words in Raw Data')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(section_data['content']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Raw Data')\n",
    "plt.show()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "section_data['Sentiment'] = section_data['content'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Sentiment Distribution')\n",
    "sns.histplot(section_data['Sentiment'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Apply the function to the raw content column\n",
    "unique_chars_before_cleaning = unique_characters_count(section_data['content'])\n",
    "print(\"Unique Characters Before Cleaning:\")\n",
    "print(unique_chars_before_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01747eb1",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Clean the textual data by removing unnecessary characters, stopwords, and lemmatizing the text. Also, remove duplicates and blank entries to ensure the quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text cleaning function\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "section_data['Cleaned Text'] = section_data['content'].progress_apply(clean_text)\n",
    "\n",
    "# Remove duplicates in the data\n",
    "print(\"Removing duplicates...\")\n",
    "section_data = section_data.drop_duplicates(subset=['Cleaned Text'])\n",
    "print(\"Duplicates removed.\")\n",
    "\n",
    "# Remove blank entries from the data\n",
    "print(\"Removing blank entries...\")\n",
    "section_data = section_data[section_data['Cleaned Text'].str.strip() != '']\n",
    "print(\"Blank entries removed.\")\n",
    "\n",
    "print(\"\\nProcessing ngrams...\")\n",
    "\n",
    "# Find frequent bigrams in the 'cleaned text' column\n",
    "print(\"\\nFrequent Bigrams in the Cleaned Text:\")\n",
    "print(find_frequent_ngrams(section_data, 'Cleaned Text', num_terms=10, ngram_size=2))\n",
    "\n",
    "# Find frequent trigrams in the 'cleaned text' column\n",
    "print(\"\\nFrequent Trigrams in the Cleaned Text:\")\n",
    "print(find_frequent_ngrams(section_data, 'Cleaned Text', num_terms=50, ngram_size=3))\n",
    "\n",
    "# Removing frequent trigrams\n",
    "print(\"\\nRemoving top frequent Trigrams based on count.  Threshold = 300\")\n",
    "section_data = remove_frequent_trigrams(section_data, 'Cleaned Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797996b",
   "metadata": {},
   "source": [
    "# Extract Category Feature from Document Text\n",
    "\n",
    "Define categories and corresponding keywords for classification. Assign categories to each document based on keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories and corresponding keywords for classification\n",
    "categories = {\n",
    "    'Installation & Setup': [\n",
    "        'install', 'setup', 'implementation', 'deployment', 'configure', 'initialization', \n",
    "        'installing', 'deploy', 'configuration', 'set-up', 'initiate', 'launch', 'activate',\n",
    "        'how to install', 'setting up', 'installation guide', 'deploying', 'configuring'\n",
    "    ],\n",
    "    'Maintenance & Management': [\n",
    "        'maintain', 'maintenance', 'servicing', 'management', 'optimization', 'service', \n",
    "        'manage', 'routine check', 'system upkeep', 'system care', 'upkeep', 'tune-up',\n",
    "        'maintaining', 'managing', 'service routine', 'optimizing', 'how to maintain'\n",
    "    ],\n",
    "    'Troubleshooting & Support': [\n",
    "        'troubleshoot', 'error', 'issue', 'problem', 'diagnosis', 'resolution', 'fix', \n",
    "        'solve', 'rectify', 'repair', 'resolve', 'correct', 'debug', 'fault finding',\n",
    "        'troubleshooting', 'solving issues', 'fixing errors', 'diagnosing problems', 'resolving'\n",
    "    ],\n",
    "    'Upgrades & Updates': [\n",
    "        'upgrade', 'update', 'new version', 'patch', 'release', 'enhancement', 'updating', \n",
    "        'upgrading', 'version upgrade', 'system update', 'software update', 'patching',\n",
    "        'how to upgrade', 'applying updates', 'version updating', 'software enhancement'\n",
    "    ],\n",
    "    'General Information & Overview': [\n",
    "        'overview', 'introduction', 'info', 'summary', 'guide', 'documentation', \n",
    "        'information', 'details', 'background', 'basics', 'general data', 'key points',\n",
    "        'what is', 'explain', 'description of', 'details about'\n",
    "    ],\n",
    "    'Security & Monitoring': [\n",
    "        'surveillance', 'log management', 'event tracking', 'real-time analysis', \n",
    "        'security watch', 'monitoring', 'security check', 'system monitoring', 'network watch',\n",
    "        'security overview', 'monitoring setup', 'event tracking system'\n",
    "    ],\n",
    "    'Threat Detection & Analysis': [\n",
    "        'threat detection', 'anomaly detection', 'intrusion detection', 'threat intelligence', \n",
    "        'security alerts', 'risk detection', 'threat identification', 'vulnerability detection', \n",
    "        'security threat detection', 'analyzing threats', 'identifying risks', 'detecting anomalies'\n",
    "    ],\n",
    "    'Incident Response & Management': [\n",
    "        'incident response', 'incident management', 'forensics', 'mitigation', 'recovery', \n",
    "        'incident handling', 'crisis management', 'incident analysis', 'emergency response',\n",
    "        'responding to incidents', 'managing incidents', 'incident recovery'\n",
    "    ],\n",
    "    'Compliance & Auditing': [\n",
    "        'compliance', 'regulatory compliance', 'audit', 'reporting', 'policy enforcement', \n",
    "        'regulation management', 'compliance tracking', 'legal compliance', 'audit management',\n",
    "        'compliance policies', 'auditing processes', 'regulatory reporting'\n",
    "    ],\n",
    "    'Integration & Compatibility': [\n",
    "        'integration', 'compatibility', 'third-party integration', 'API', 'interoperability', \n",
    "        'system merging', 'software integration', 'data integration', 'platform integration',\n",
    "        'integrating systems', 'API usage', 'compatibility issues'\n",
    "    ],\n",
    "    'Network Security & Protection': [\n",
    "        'network security', 'firewall', 'traffic analysis', 'intrusion prevention', \n",
    "        'network protection', 'cybersecurity', 'network defense', 'network safeguard',\n",
    "        'protecting networks', 'network firewalls', 'cybersecurity measures'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874607fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to assign categories to each document\n",
    "section_data['Category'] = section_data['content'].apply(lambda text: assign_category(text, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005548d",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis on Cleaned Text\n",
    "\n",
    "Repeat the EDA process on the cleaned text data. This includes analyzing the text length distribution, word frequency, word cloud, sentiment, and distribution of assigned categories from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the DataFrame after text cleaning\n",
    "describe_dataframe(section_data)\n",
    "\n",
    "# Check for duplicate text\n",
    "duplicate_text = section_data[section_data.duplicated(['content'])]\n",
    "\n",
    "# Print the duplicates\n",
    "print(\"Duplicate Text Entries:\")\n",
    "print(duplicate_text)\n",
    "\n",
    "# Check for missing data in each column\n",
    "missing_data = section_data.isnull().sum()\n",
    "\n",
    "# Print columns with missing data\n",
    "print(\"Columns with Missing Data:\")\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "# Analyze text length distribution after cleaning\n",
    "section_data['Cleaned Text Length'] = section_data['Cleaned Text'].apply(len)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Text Length Distribution After Cleaning')\n",
    "sns.histplot(section_data['Cleaned Text Length'], bins=50, kde=True)\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Perform word frequency analysis on cleaned text\n",
    "freq_dist_cleaned = FreqDist(word_tokenize(' '.join(section_data['Cleaned Text'])))\n",
    "plot_word_frequencies(freq_dist_cleaned, 'Top Words in Cleaned Data')\n",
    "\n",
    "# Generate a word cloud for cleaned text\n",
    "wordcloud_cleaned = WordCloud(width=800, height=400, background_color='white').generate(' '.join(section_data['Cleaned Text']))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Cleaned Data')\n",
    "plt.show()\n",
    "\n",
    "# Perform sentiment analysis on cleaned text\n",
    "section_data['Cleaned Sentiment'] = section_data['Cleaned Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Sentiment Distribution After Cleaning')\n",
    "sns.histplot(section_data['Cleaned Sentiment'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Display the distribution of categories\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='Category', data=section_data)\n",
    "plt.title('Distribution of Assigned Categories from Text')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Category')\n",
    "plt.show()\n",
    "\n",
    "# Find frequent bigrams in the 'cleaned text' column\n",
    "print(\"\\nFrequent Bigrams in Cleaned Text after Trigram removal:\")\n",
    "print(find_frequent_ngrams(section_data, 'Cleaned Text', num_terms=10, ngram_size=2))\n",
    "\n",
    "# Find frequent trigrams in the 'cleaned text' column\n",
    "print(\"\\nFrequent Trigrams in Cleaned Text after Trigram removal:\")\n",
    "print(find_frequent_ngrams(section_data, 'Cleaned Text', num_terms=50, ngram_size=3))\n",
    "\n",
    "# Apply the function to get unique characters in the cleaned content column\n",
    "unique_chars_after_cleaning = unique_characters_count(section_data['Cleaned Text'])\n",
    "print(\"\\nUnique Characters After Cleaning:\")\n",
    "print(unique_chars_after_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab9c6f",
   "metadata": {},
   "source": [
    "### Saving Cleaned Data\n",
    "\n",
    "After the analysis, save the cleaned and processed data to a new CSV file for future use or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "section_data.to_csv('cleaned_section_data_with_categories.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
