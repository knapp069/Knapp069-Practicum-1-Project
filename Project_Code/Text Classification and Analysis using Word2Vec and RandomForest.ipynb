{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f52ec6",
   "metadata": {},
   "source": [
    "# Text Classification and Analysis using Word2Vec and RandomForest\n",
    "\n",
    "In this notebook, we perform text classification on a dataset using Word2Vec for word embeddings and RandomForest for classification. The process includes text processing, feature extraction, handling class imbalance with SMOTE, training the classifier, and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import joblib\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer for word tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a10ba7",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "Load the cleaned textual data and preprocess it for classification. This involves filling missing values and defining category rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ba1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data from the CSV file\n",
    "df = pd.read_csv('cleaned_section_data.csv')\n",
    "df['Cleaned Text'] = df['Cleaned Text'].fillna('')  # Fill missing text with empty strings\n",
    "\n",
    "# Define categories and corresponding keywords for classification\n",
    "categories = {\n",
    "    'Installation & Setup': [\n",
    "        'install', 'setup', 'implementation', 'deployment', 'configure', 'initialization', \n",
    "        'installing', 'deploy', 'configuration', 'set-up', 'initiate', 'launch', 'activate',\n",
    "        'how to install', 'setting up', 'installation guide', 'deploying', 'configuring'\n",
    "    ],\n",
    "    'Maintenance & Management': [\n",
    "        'maintain', 'maintenance', 'servicing', 'management', 'optimization', 'service', \n",
    "        'manage', 'routine check', 'system upkeep', 'system care', 'upkeep', 'tune-up',\n",
    "        'maintaining', 'managing', 'service routine', 'optimizing', 'how to maintain'\n",
    "    ],\n",
    "    'Troubleshooting & Support': [\n",
    "        'troubleshoot', 'error', 'issue', 'problem', 'diagnosis', 'resolution', 'fix', \n",
    "        'solve', 'rectify', 'repair', 'resolve', 'correct', 'debug', 'fault finding',\n",
    "        'troubleshooting', 'solving issues', 'fixing errors', 'diagnosing problems', 'resolving'\n",
    "    ],\n",
    "    'Upgrades & Updates': [\n",
    "        'upgrade', 'update', 'new version', 'patch', 'release', 'enhancement', 'updating', \n",
    "        'upgrading', 'version upgrade', 'system update', 'software update', 'patching',\n",
    "        'how to upgrade', 'applying updates', 'version updating', 'software enhancement'\n",
    "    ],\n",
    "    'General Information & Overview': [\n",
    "        'overview', 'introduction', 'info', 'summary', 'guide', 'documentation', \n",
    "        'information', 'details', 'background', 'basics', 'general data', 'key points',\n",
    "        'what is', 'explain', 'description of', 'details about'\n",
    "    ],\n",
    "    'Security & Monitoring': [\n",
    "        'surveillance', 'log management', 'event tracking', 'real-time analysis', \n",
    "        'security watch', 'monitoring', 'security check', 'system monitoring', 'network watch',\n",
    "        'security overview', 'monitoring setup', 'event tracking system'\n",
    "    ],\n",
    "    'Threat Detection & Analysis': [\n",
    "        'threat detection', 'anomaly detection', 'intrusion detection', 'threat intelligence', \n",
    "        'security alerts', 'risk detection', 'threat identification', 'vulnerability detection', \n",
    "        'security threat detection', 'analyzing threats', 'identifying risks', 'detecting anomalies'\n",
    "    ],\n",
    "    'Incident Response & Management': [\n",
    "        'incident response', 'incident management', 'forensics', 'mitigation', 'recovery', \n",
    "        'incident handling', 'crisis management', 'incident analysis', 'emergency response',\n",
    "        'responding to incidents', 'managing incidents', 'incident recovery'\n",
    "    ],\n",
    "    'Compliance & Auditing': [\n",
    "        'compliance', 'regulatory compliance', 'audit', 'reporting', 'policy enforcement', \n",
    "        'regulation management', 'compliance tracking', 'legal compliance', 'audit management',\n",
    "        'compliance policies', 'auditing processes', 'regulatory reporting'\n",
    "    ],\n",
    "    'Integration & Compatibility': [\n",
    "        'integration', 'compatibility', 'third-party integration', 'API', 'interoperability', \n",
    "        'system merging', 'software integration', 'data integration', 'platform integration',\n",
    "        'integrating systems', 'API usage', 'compatibility issues'\n",
    "    ],\n",
    "    'Network Security & Protection': [\n",
    "        'network security', 'firewall', 'traffic analysis', 'intrusion prevention', \n",
    "        'network protection', 'cybersecurity', 'network defense', 'network safeguard',\n",
    "        'protecting networks', 'network firewalls', 'cybersecurity measures'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to assign a category to a text based on keywords\n",
    "def assign_category(text, categories):\n",
    "    text = str(text).lower()\n",
    "    category_scores = {category: 0 for category in categories.keys()}\n",
    "    for category, keywords in categories.items():\n",
    "        category_scores[category] += sum(text.count(keyword) for keyword in keywords)\n",
    "    assigned_category = max(category_scores, key=category_scores.get)\n",
    "    return 'Other' if category_scores[assigned_category] == 0 else assigned_category\n",
    "\n",
    "# Apply the function to assign categories to each document\n",
    "df['Category'] = df['Cleaned Text'].apply(lambda text: assign_category(text, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93548dc0",
   "metadata": {},
   "source": [
    "### Data Filtering and Tokenization\n",
    "\n",
    "Filter out categories with insufficient samples and tokenize the text for the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69702e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for the minimum number of samples required in a category\n",
    "min_samples_threshold = 3  \n",
    "\n",
    "# Count the number of samples in each category and identify categories to exclude\n",
    "category_counts = df['Category'].value_counts()\n",
    "categories_to_exclude = category_counts[category_counts < min_samples_threshold].index.tolist()\n",
    "\n",
    "# Filter out categories with insufficient samples\n",
    "df_filtered = df[~df['Category'].isin(categories_to_exclude)]\n",
    "\n",
    "# Tokenize the text data for Word2Vec model training\n",
    "tokenized_text = [word_tokenize(text) for text in df_filtered['Cleaned Text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878cad6",
   "metadata": {},
   "source": [
    "### Word2Vec Model Training\n",
    "\n",
    "Train a Word2Vec model on the tokenized text to create word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8402272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_text, vector_size=200, window=10, min_count=1, workers=4)\n",
    "\n",
    "# Function to create a feature vector for a document by averaging its word vectors\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Filter out words not in the model's vocabulary\n",
    "    doc = [word for word in doc if word in word2vec_model.wv]\n",
    "    # Return the mean of the word vectors if the document is not empty, else return a zero vector\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0) if doc else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Create feature vectors for each document using the Word2Vec model\n",
    "w2v_feature_vectors = np.array([document_vector(word2vec_model, doc) for doc in tokenized_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c78fc7",
   "metadata": {},
   "source": [
    "### Feature Extraction with TF-IDF and Combining Features\n",
    "\n",
    "Extract features using TF-IDF and combine them with Word2Vec features for a more robust feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58daf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "tfidf_feature_vectors = tfidf_vectorizer.fit_transform(df_filtered['Cleaned Text']).toarray()\n",
    "\n",
    "# Combine Word2Vec and TF-IDF features\n",
    "combined_features = np.hstack((w2v_feature_vectors, tfidf_feature_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6013efd",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance with SMOTE\n",
    "\n",
    "Use SMOTE to handle class imbalance in the dataset, creating synthetic samples for under-represented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE and apply it to the combined feature set\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(combined_features, df_filtered['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6bcf16",
   "metadata": {},
   "source": [
    "### Data Splitting and Classifier Training\n",
    "\n",
    "Split the data into training and testing sets and train a RandomForest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafcedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a RandomForest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa90c6",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the classifier using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d08d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Print a classification report to evaluate the classifier\n",
    "print(classification_report(y_test, predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d92b3dd",
   "metadata": {},
   "source": [
    "### Saving Models and Results\n",
    "\n",
    "Save the trained models and processed data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773de8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF model, Word2Vec model, combined features, and the filtered dataset\n",
    "joblib.dump(tfidf_vectorizer, \"tfidf_model.pkl\")\n",
    "word2vec_model.save('word2vec_model.bin')\n",
    "np.save('combined_features.npy', combined_features)\n",
    "df_filtered.to_csv('cleaned_section_data_with_categories.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
