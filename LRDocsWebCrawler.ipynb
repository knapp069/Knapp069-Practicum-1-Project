{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fef5c9f",
   "metadata": {},
   "source": [
    "# Web Scraper for docs.logrhythm.com\n",
    "\n",
    "This notebook contains a script to crawl docs.logrhythm.com, focusing on English content within the domain. The script scrapes data from the website and stores it in MongoDB, avoiding duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f6e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from langdetect import detect, DetectorFactory\n",
    "import pymongo\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7748dbd",
   "metadata": {},
   "source": [
    "## MongoDB Connection Setup\n",
    "\n",
    "Establishing a connection to MongoDB and defining the database and collections to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee9a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"WS_Data_DB\"]  # Database name\n",
    "collection = db[\"LogRhythm7_15Docs\"]  # Collection name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311a7f7",
   "metadata": {},
   "source": [
    "## Initial Setup for Language Detection\n",
    "\n",
    "Set the seed for the language detection library to ensure consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d4b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for consistent language detection\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa675650",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "\n",
    "Defining the functions needed for web crawling. This includes functions to get page content, check robots.txt compliance, extract information from HTML, detect if the content is English, and extract valid links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310e6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch page content with retry logic\n",
    "def get_page_content(url, max_retries=3, delay=1):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response.content\n",
    "            else:\n",
    "                print(f\"Failed to fetch content. Status code: {response.status_code}\")\n",
    "                retries += 1\n",
    "                time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching content: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "# Function to check robots.txt compliance\n",
    "def is_allowed_by_robots(url):\n",
    "    base_url = urljoin(url, '/')\n",
    "    if not base_url.endswith('/'):\n",
    "        base_url += '/'\n",
    "    robots_txt_url = urljoin(base_url, 'robots.txt')\n",
    "    try:\n",
    "        response = requests.get(robots_txt_url)\n",
    "        response.raise_for_status()\n",
    "        robots_txt_content = response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching robots.txt: {e}\")\n",
    "        return False\n",
    "    rp = RobotFileParser()\n",
    "    rp.parse(robots_txt_content.splitlines())\n",
    "    return rp.can_fetch(\"*\", url)\n",
    "\n",
    "# Function to extract information from HTML content\n",
    "def extract_information(html_content, url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    # Extract the title\n",
    "    title = soup.title.string if soup.title else 'No Title'\n",
    "\n",
    "    # Extract sectioned content\n",
    "    sectioned_content = extract_sectioned_content(html_content)\n",
    "\n",
    "    # Create the document to be stored in MongoDB\n",
    "    document = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"content_sections\": sectioned_content,\n",
    "        \"retrieved_on\": datetime.utcnow()\n",
    "    }\n",
    "\n",
    "    return document\n",
    "\n",
    "# Function to extract sectioned content\n",
    "def extract_sectioned_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    sections = {}\n",
    "    current_header = 'General'  # Default section if no headers are found\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'p', 'li', 'table']):\n",
    "        if element.name in ['h1', 'h2', 'h3']:\n",
    "            current_header = element.get_text(strip=True)\n",
    "            sections[current_header] = ''  # Initialize the section with an empty string\n",
    "        else:\n",
    "            # Ensure the current header exists in the dictionary before appending content\n",
    "            if current_header not in sections:\n",
    "                sections[current_header] = ''\n",
    "            if element.name == 'table':\n",
    "                table_text = process_table(element)\n",
    "                sections[current_header] += table_text\n",
    "            else:\n",
    "                sections[current_header] += element.get_text(separator='\\n', strip=True) + '\\n'\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Function to process found table information\n",
    "def process_table(table_tag):\n",
    "    table_text = ''\n",
    "    for row in table_tag.find_all('tr'):\n",
    "        row_cells = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]\n",
    "        row_text = ', '.join(row_cells)\n",
    "        table_text += row_text + '\\n'\n",
    "    return table_text\n",
    "\n",
    "# Function to check if content is in English\n",
    "def is_english_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meaningful_text = soup.body.get_text(separator=' ', strip=True) if soup.body else ''\n",
    "    try:\n",
    "        return detect(meaningful_text[:1000]) == 'en'  # Sample first 1000 characters\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to extract valid links\n",
    "def extract_links(html_content, base_url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    valid_links = []\n",
    "    for link in links:\n",
    "        if not link.startswith('http'):\n",
    "            link = urljoin(base_url, link)\n",
    "        parsed_link = urlparse(link)\n",
    "        if parsed_link.netloc == 'docs.logrhythm.com' and parsed_link.path.startswith('/lrsiem/7.15.0/'):\n",
    "            valid_links.append(link)\n",
    "    return valid_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35539b9",
   "metadata": {},
   "source": [
    "## Crawl Function\n",
    "\n",
    "Define the crawl function to recursively crawl the website, adhering to the specified rules such as staying within the docs.logrhythm.com domain and processing only English content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1426b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive crawl function with rate limiting\n",
    "def crawl(url, visited, delay=0.5):\n",
    "    if url in visited or not url.startswith('http'):\n",
    "        return\n",
    "    visited.add(url)\n",
    "    time.sleep(delay)  # Rate limiting\n",
    "    if is_allowed_by_robots(url) and urlparse(url).netloc == 'docs.logrhythm.com':\n",
    "        content = get_page_content(url)\n",
    "        if content and is_english_content(content):\n",
    "            document = extract_information(content, url)\n",
    "            if collection.count_documents({\"url\": url}) == 0:\n",
    "                collection.insert_one(document)\n",
    "            for link in extract_links(content, url):\n",
    "                crawl(link, visited, delay)\n",
    "    else:\n",
    "        print(f\"Crawling not allowed or outside domain for: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4c8f3",
   "metadata": {},
   "source": [
    "## Start Crawling\n",
    "\n",
    "Initiate the web crawler from the main page of docs.logrhythm.com. The crawler will recursively visit each link, staying within the specified domain and processing only English content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34283c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Crawling\n",
    "start_url = \"https://docs.logrhythm.com/lrsiem/7.15.0/\"\n",
    "visited_urls = set()  # Set of visited URLs to avoid revisiting\n",
    "crawl(start_url, visited_urls)  # Start the web scraping process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
