{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238c5c6c",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "\n",
    "This notebook contains the steps for fetching and preprocessing text data from a MongoDB database for NLP tasks. The preprocessing includes cleaning the text, removing unnecessary characters, tokenization, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e814975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import warnings\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5cfb3",
   "metadata": {},
   "source": [
    "## Download Necessary NLTK Data\n",
    "\n",
    "Downloading required NLTK packages for stopwords, tokenization, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38d835d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2a92d",
   "metadata": {},
   "source": [
    "## MongoDB Connection and Data Retrieval\n",
    "\n",
    "Establishing a connection to MongoDB and retrieving documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac823841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"WS_Data_DB\"]  # Database name\n",
    "collection = db[\"LogRhythmDocs\"]  # Collection name\n",
    "\n",
    "# Fetch data from MongoDB\n",
    "documents = collection.find()\n",
    "df = pd.DataFrame(list(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8cc2d",
   "metadata": {},
   "source": [
    "## Text Preprocessing Function\n",
    "\n",
    "Defining a function for preprocessing the text data. This includes HTML tag removal, lowercasing, removing URLs and special characters, tokenization, stopwords removal, optional spell correction, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77dd7f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    processed_content\n",
      "0   show navig go homepag logrhythm com communiti ...\n",
      "4   logrhythm axon show navig go homepag axon logr...\n",
      "6   axon prerequisit consider show navig go homepa...\n",
      "9   axon agent show navig go homepag axon logrhyth...\n",
      "11  axon administr guid show navig go homepag axon...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33448\\3964156228.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Populate intent DataFrame from n-grams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ngrams'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# Append rows to intent_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mintent_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mintent_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'intent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'example'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Write grouped examples to YAML files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouped_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Spellchecker instance\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Suppress Wanings for Beautifulsoup\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Define function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Lowercasing the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Define a list of repetitive phrases to remove\n",
    "    repetitive_phrases = [\n",
    "        \"show navigation go homepage\", \n",
    "        \"logrhythm documentation\",\n",
    "        \"skip to main content\"\n",
    "    ]\n",
    "    \n",
    "    # Remove repetitive phrases\n",
    "    for phrase in repetitive_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "\n",
    "    # Remove URLs, special characters, and numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Filter out None values, empty strings, and whitespace-only strings\n",
    "    words = [word for word in words if word and not word.isspace()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Snowball stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Generate n-grams\n",
    "    n = 5\n",
    "    n_grams = [' '.join(gram) for gram in ngrams(stemmed_words, n)]\n",
    "\n",
    "    # Combine back to text\n",
    "    processed_text = ' '.join(n_grams)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to each document's content\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Remove None entries and duplicates after preprocessing\n",
    "df = df.dropna(subset=['processed_content'])\n",
    "df = df.drop_duplicates(subset=['processed_content'])\n",
    "\n",
    "# Display the processed content\n",
    "print(df[['processed_content']].head())\n",
    "\n",
    "# Define directory to save YAML files\n",
    "output_dir = \"C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize an empty list to store rows\n",
    "intent_rows = []\n",
    "\n",
    "# Populate intent DataFrame from n-grams\n",
    "for index, row in df.iterrows():\n",
    "    for ngram in row['ngrams']:\n",
    "        # Append rows to intent_rows list\n",
    "        intent_rows.append({'intent': row['_id'], 'example': ngram})\n",
    "\n",
    "# Create DataFrame from list of rows\n",
    "intent_df = pd.DataFrame(intent_rows)\n",
    "\n",
    "# Write grouped examples to YAML files\n",
    "for intent, examples in intent_df.groupby('intent')['example']:\n",
    "    with open(os.path.join(output_dir, f\"{intent}.yml\"), 'w') as f:\n",
    "        f.write('\\n'.join(examples))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
