{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb4ceb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Preprocessing Documents: 100%|██████████| 1084/1084 [00:02<00:00, 487.17it/s]\n",
      "Detecting Language and Filtering: 100%|██████████| 1084/1084 [00:13<00:00, 77.92it/s] \n",
      "Removing Stopwords: 100%|██████████| 1084/1084 [00:00<00:00, 1771.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\ted59\\Knapp069-Practicum-1-Project\\Processed Data\\processed_document_data.pkl and C:\\Users\\ted59\\Knapp069-Practicum-1-Project\\Processed Data\\processed_document_data.txt\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from bs4 import BeautifulSoup, Comment, MarkupResemblesLocatorWarning\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from langdetect import detect, DetectorFactory\n",
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download Necessary NLTK Data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n",
    "# MongoDB Connection and Data Retrieval\n",
    "try:\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"WS_Data_DB\"]\n",
    "    collection = db[\"LogRhythm7_15Docs\"]\n",
    "    # Fetch documents from MongoDB\n",
    "    documents = list(collection.find())\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")\n",
    "    documents = []\n",
    "\n",
    "# Function to remove HTML, JavaScript, and CSS\n",
    "def clean_html_and_js(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    try:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "        return soup.get_text(separator=' ').strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in cleaning HTML/JS: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Phrase Normalization Function\n",
    "def normalize_phrases(text):\n",
    "    phrase_map = {\n",
    "        'logrhythm siem': 'LogRhythm SIEM',\n",
    "        'logrhythm.com': 'LogRhythm',\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    for key, value in phrase_map.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "# Improved regex pattern to remove punctuation while keeping certain cases\n",
    "pattern = r\"\"\"\n",
    "    (?<![0-9a-zA-Z])[\\/,(](?![0-9a-zA-Z])|        # Keeps slashes, commas, and parentheses not surrounded by alphanumeric characters\n",
    "    (?<!\\b(?:i\\.e|e\\.g)\\.)(?<!\\b(?:i\\.e|e\\.g)),|  # Keeps commas not preceded by 'i.e.' or 'e.g.'\n",
    "    (?<=\\d)[\\/:](?=\\d)|                           # Keeps slashes and colons between numbers\n",
    "    (?<!n)[/](?!a)|                               # Keeps slash between 'n' and 'a' in 'n/a'\n",
    "    (?<=\\s)[.:,;!?](?=\\s|$)|                      # Removes punctuation with a space before and after it or at the end of a string\n",
    "    (?<=\\s\\w)\\.(?=\\s|$)                           # Removes period with a letter before it and a space after it or at the end of a string\n",
    "\"\"\"\n",
    "\n",
    "# Lemmatization Function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = clean_html_and_js(text)\n",
    "    text = normalize_phrases(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Preserve regex patterns\n",
    "    preserved_regex = []\n",
    "    text = re.sub(r'\\/.*?\\/[a-z]*', lambda match: preserved_regex.append(match.group()) or f'<<{len(preserved_regex) - 1}>>', text)\n",
    "    text = re.sub(pattern, ' ', text, flags=re.VERBOSE)\n",
    "    # Restore preserved regex patterns\n",
    "    for i, regex_pattern in enumerate(preserved_regex):\n",
    "        text = text.replace(f'<<{i}>>', regex_pattern)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    return text.lower()  # Convert text to lowercase\n",
    "\n",
    "# Function for Data Quality Check\n",
    "def data_quality_check(texts):\n",
    "    # Implement checks here (e.g., checking length, unexpected characters)\n",
    "    pass\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in language detection: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Function to find frequent trigrams from the raw text\n",
    "def find_frequent_trigrams(texts, threshold=5):\n",
    "    ngram_counter = Counter()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        trigrams = ngrams(words, 3)\n",
    "        ngram_counter.update([' '.join(gram) for gram in trigrams])\n",
    "    return sorted([(gram, count) for gram, count in ngram_counter.items() if count >= threshold], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Preprocess documents and extract raw text\n",
    "raw_texts = []\n",
    "for doc in tqdm(documents, desc=\"Preprocessing Documents\"):\n",
    "    try:\n",
    "        raw_text = ' '.join(section_text for section_text in doc['content_sections'].values())\n",
    "        raw_texts.append(preprocess_text(raw_text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing document: {e}\")\n",
    "\n",
    "# Implement data quality check\n",
    "data_quality_check(raw_texts)        \n",
    "        \n",
    "# Language Detection and Filtering\n",
    "DetectorFactory.seed = 0\n",
    "filtered_texts = []\n",
    "for text in tqdm(raw_texts, desc=\"Detecting Language and Filtering\"):\n",
    "    if detect_language(text) == 'en':\n",
    "        filtered_texts.append(text)\n",
    "\n",
    "# Removing stopwords and finding frequent trigrams\n",
    "filtered_texts = [remove_stopwords(text) for text in tqdm(filtered_texts, desc=\"Removing Stopwords\")]\n",
    "frequent_trigrams = find_frequent_trigrams(filtered_texts)\n",
    "\n",
    "# Saving results to DataFrame\n",
    "df = pd.DataFrame(frequent_trigrams, columns=['Trigram', 'Count'])\n",
    "\n",
    "# Saving DataFrame to Pickle and Text Files\n",
    "pickle_path = 'C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\\\\Processed Data\\\\processed_document_data.pkl'\n",
    "text_path = 'C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\\\\Processed Data\\\\processed_document_data.txt'\n",
    "\n",
    "df.to_pickle(pickle_path)\n",
    "df.to_csv(text_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {pickle_path} and {text_path}\")\n",
    "\n",
    "# Creating dictionary and corpus for LDA\n",
    "dictionary = gensim.corpora.Dictionary(df['Trigram'].apply(lambda x: x.split()))\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in filtered_texts]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=5, passes=2, workers=2)\n",
    "\n",
    "# Save LDA model\n",
    "lda_model.save('C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\\\\Processed Data\\\\lda_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a16783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac6d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
