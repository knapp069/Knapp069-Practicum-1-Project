{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deae042",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "This notebook outlines the steps for fetching and preprocessing text data from a MongoDB database for NLP tasks. It includes cleaning the text, removing unnecessary characters, tokenization, lemmatization, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edba2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "from langdetect import detect, DetectorFactory\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce3eda",
   "metadata": {},
   "source": [
    "## Downloading Necessary NLTK Data\n",
    "The following cells download the required NLTK packages for stopwords, tokenization, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fee5056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ted59\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Necessary NLTK Data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37850f3a",
   "metadata": {},
   "source": [
    "## MongoDB Connection and Data Retrieval\n",
    "Establish a connection to MongoDB and retrieve documents from the specified collection for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbcfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"WS_Data_DB\"]  # Database name\n",
    "collection = db[\"LogRhythm7_15Docs\"]  # Collection name\n",
    "\n",
    "# Fetch data from MongoDB\n",
    "documents = collection.find().limit(100)  #<--------------------------------\n",
    "df = pd.DataFrame(list(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c07a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.cursor.Cursor object at 0x000002F4A5E97990>\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dabb6",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "Define functions for preprocessing the text data, including HTML tag removal, lowercasing, removing URLs and special characters, tokenization, stopwords removal, lemmatization, and generating n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86136b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress Warnings for BeautifulSoup\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Function to remove HTML, JavaScript, and CSS\n",
    "def clean_html_and_js(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text.strip()\n",
    "\n",
    "# Function to parse the user input into indices, including ranges\n",
    "def parse_input_indices(input_str):\n",
    "    indices = set()\n",
    "    if not input_str.strip():\n",
    "        return list(indices)  # Return an empty list if input is empty\n",
    "\n",
    "    parts = input_str.split(',')\n",
    "\n",
    "    for part in parts:\n",
    "        try:\n",
    "            if '-' in part:\n",
    "                start, end = map(int, part.split('-'))\n",
    "                indices.update(range(start, end + 1))\n",
    "            else:\n",
    "                indices.add(int(part.strip()))\n",
    "        except ValueError:\n",
    "            print(f\"Invalid input: {part}. Please enter valid indices.\")\n",
    "            continue\n",
    "\n",
    "    return list(indices)\n",
    "\n",
    "# Make boilerplate phrase list\n",
    "def populate_boilerplate_phrases(frequent_phrases, input_indices):\n",
    "    return [frequent_phrases[i-1][0] for i in input_indices if i <= len(frequent_phrases) and i > 0]\n",
    "\n",
    "# Function to display phrases and ask for removal after every 20 phrases\n",
    "def display_and_select_phrases(phrases):\n",
    "    selected_phrases = []\n",
    "    phrases_per_batch = 20\n",
    "\n",
    "    print(f\"Total phrases: {len(phrases)}\")  # Debug: Print the total number of phrases\n",
    "\n",
    "    for i in range(0, len(phrases), phrases_per_batch):\n",
    "        batch = phrases[i:i+phrases_per_batch]\n",
    "        print(\"\\nFrequent Phrases:\")\n",
    "        for j, (phrase, count) in enumerate(batch, 1):\n",
    "            print(f\"{i+j}. {phrase} (Count: {count})\")\n",
    "\n",
    "        input_str = input(\"\\nEnter the indices or ranges of indices (e.g., 1-3, 5, 7) you want to remove from this batch: \")\n",
    "        input_indices = parse_input_indices(input_str)\n",
    "        selected_phrases.extend(populate_boilerplate_phrases(batch, input_indices))\n",
    "\n",
    "    return selected_phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e6cf0",
   "metadata": {},
   "source": [
    "## Extracting Frequent Phrases\n",
    "Process the fetched documents to extract and display frequent phrases for user selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c737a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Phrases: []\n",
      "Total phrases: 0\n",
      "Selected phrases for removal: []\n"
     ]
    }
   ],
   "source": [
    "# Define a function to find frequent phrases from the documents\n",
    "def find_frequent_phrases(documents, threshold=1):  \n",
    "    phrase_counter = Counter()\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Make sure to replace 'text_field' with your actual field name\n",
    "        text = clean_html_and_js(doc['content_sections'])\n",
    "        words = word_tokenize(text.lower())\n",
    "        # Count bigrams in the document\n",
    "        phrase_counter.update([' '.join(pair) for pair in ngrams(words, 2)])\n",
    "\n",
    "    frequent_phrases = [(phrase, count) for phrase, count in phrase_counter.items() if count >= threshold]\n",
    "    return frequent_phrases\n",
    "\n",
    "frequent_phrases = find_frequent_phrases(documents)\n",
    "print(\"Frequent Phrases:\", frequent_phrases[:10])  # Print the top 10 frequent phrases to check\n",
    "\n",
    "\n",
    "# Display and select phrases to remove\n",
    "selected_phrases_for_removal = display_and_select_phrases(frequent_phrases)\n",
    "print(\"Selected phrases for removal:\", selected_phrases_for_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f3da5",
   "metadata": {},
   "source": [
    "## Preprocessing Document Sections\n",
    "Apply the preprocessing steps to each section in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43439000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ted59\\AppData\\Local\\Temp\\ipykernel_17128\\731372216.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "C:\\Users\\ted59\\AppData\\Local\\Temp\\ipykernel_17128\\731372216.py:6: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = clean_html_and_js(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to preprocess each section in a document\n",
    "def preprocess_document_sections(document_sections):\n",
    "    processed_sections = {}\n",
    "    for section, content in document_sections.items():\n",
    "        processed_text = preprocess_text(content)\n",
    "        processed_sections[section] = processed_text\n",
    "    return processed_sections\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_content_sections'] = df['content_sections'].apply(preprocess_document_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62e06c",
   "metadata": {},
   "source": [
    "## Language Detection and Filtering\n",
    "Detect the language of each document and filter out non-English documents to maintain consistency in language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c702f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Detection Setup\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Detect language for each document\n",
    "df['language'] = df['processed_content_sections'].apply(lambda x: detect_language(' '.join(x.values())))\n",
    "\n",
    "# Keep only English language documents\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5703e42",
   "metadata": {},
   "source": [
    "## Saving Processed Data\n",
    "Save the processed DataFrame to a Pickle file for future use and also export it as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f93c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DataFrame to Pickle and Text Files\n",
    "df.to_pickle('C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\\\\Processed Data\\\\processed_document_data.pkl')\n",
    "df_string = df.to_string()\n",
    "with open('C:\\\\Users\\\\ted59\\\\Knapp069-Practicum-1-Project\\\\Processed Data\\\\processed_document_data.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(df_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
