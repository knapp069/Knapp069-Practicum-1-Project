{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5391b2d6",
   "metadata": {},
   "source": [
    "# LogRhythm Chatbot for Document Query and Summarization\n",
    "\n",
    "This notebook presents a NLP-based chatbot that utilizes Word2Vec, TF-IDF, spaCy, and Hugging Face Transformers to analyze, categorize, and summarize textual data from LogRhythm documentation. It aims to provide relevant responses to user queries by identifying the most appropriate category and document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import heapq\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from summa import keywords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d91d2c5",
   "metadata": {},
   "source": [
    "### Download libraries, traing models, and cleaned data used in the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Loading spaCy model for extractive summarization\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initializing Hugging Face pipeline for abstractive summarization\n",
    "abstractive_summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Loading the trained Word2Vec model and TF-IDF vectorizer\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "tfidf_vectorizer = joblib.load(\"tfidf_model.pkl\")\n",
    "\n",
    "# Loading the cleaned data with categories\n",
    "df = pd.read_csv('cleaned_section_data_with_categories.csv')\n",
    "\n",
    "# Loading the combined embeddings\n",
    "combined_embeddings = np.load('combined_features.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9b175",
   "metadata": {},
   "source": [
    "### Setting Up Categories and Keyword Rules\n",
    "\n",
    "Define categories and corresponding keywords to classify user inputs effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining categories and their associated keywords\n",
    "categories = {\n",
    "    'Installation & Setup': [\n",
    "        'install', 'setup', 'implementation', 'deployment', 'configure', 'initialization', \n",
    "        'installing', 'deploy', 'configuration', 'set-up', 'initiate', 'launch', 'activate',\n",
    "        'how to install', 'setting up', 'installation guide', 'deploying', 'configuring'\n",
    "    ],\n",
    "    'Maintenance & Management': [\n",
    "        'maintain', 'maintenance', 'servicing', 'management', 'optimization', 'service', \n",
    "        'manage', 'routine check', 'system upkeep', 'system care', 'upkeep', 'tune-up',\n",
    "        'maintaining', 'managing', 'service routine', 'optimizing', 'how to maintain'\n",
    "    ],\n",
    "    'Troubleshooting & Support': [\n",
    "        'troubleshoot', 'error', 'issue', 'problem', 'diagnosis', 'resolution', 'fix', \n",
    "        'solve', 'rectify', 'repair', 'resolve', 'correct', 'debug', 'fault finding',\n",
    "        'troubleshooting', 'solving issues', 'fixing errors', 'diagnosing problems', 'resolving'\n",
    "    ],\n",
    "    'Upgrades & Updates': [\n",
    "        'upgrade', 'update', 'new version', 'patch', 'release', 'enhancement', 'updating', \n",
    "        'upgrading', 'version upgrade', 'system update', 'software update', 'patching',\n",
    "        'how to upgrade', 'applying updates', 'version updating', 'software enhancement'\n",
    "    ],\n",
    "    'General Information & Overview': [\n",
    "        'overview', 'introduction', 'info', 'summary', 'guide', 'documentation', \n",
    "        'information', 'details', 'background', 'basics', 'general data', 'key points',\n",
    "        'what is', 'explain', 'description of', 'details about'\n",
    "    ],\n",
    "    'Security & Monitoring': [\n",
    "        'surveillance', 'log management', 'event tracking', 'real-time analysis', \n",
    "        'security watch', 'monitoring', 'security check', 'system monitoring', 'network watch',\n",
    "        'security overview', 'monitoring setup', 'event tracking system'\n",
    "    ],\n",
    "    'Threat Detection & Analysis': [\n",
    "        'threat detection', 'anomaly detection', 'intrusion detection', 'threat intelligence', \n",
    "        'security alerts', 'risk detection', 'threat identification', 'vulnerability detection', \n",
    "        'security threat detection', 'analyzing threats', 'identifying risks', 'detecting anomalies'\n",
    "    ],\n",
    "    'Incident Response & Management': [\n",
    "        'incident response', 'incident management', 'forensics', 'mitigation', 'recovery', \n",
    "        'incident handling', 'crisis management', 'incident analysis', 'emergency response',\n",
    "        'responding to incidents', 'managing incidents', 'incident recovery'\n",
    "    ],\n",
    "    'Compliance & Auditing': [\n",
    "        'compliance', 'regulatory compliance', 'audit', 'reporting', 'policy enforcement', \n",
    "        'regulation management', 'compliance tracking', 'legal compliance', 'audit management',\n",
    "        'compliance policies', 'auditing processes', 'regulatory reporting'\n",
    "    ],\n",
    "    'Integration & Compatibility': [\n",
    "        'integration', 'compatibility', 'third-party integration', 'API', 'interoperability', \n",
    "        'system merging', 'software integration', 'data integration', 'platform integration',\n",
    "        'integrating systems', 'API usage', 'compatibility issues'\n",
    "    ],\n",
    "    'Network Security & Protection': [\n",
    "        'network security', 'firewall', 'traffic analysis', 'intrusion prevention', \n",
    "        'network protection', 'cybersecurity', 'network defense', 'network safeguard',\n",
    "        'protecting networks', 'network firewalls', 'cybersecurity measures'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to determine the category of user input based on keywords\n",
    "def determine_category(user_input):\n",
    "    for category, keywords in categories.items():\n",
    "        if any(keyword in user_input.lower() for keyword in keywords):\n",
    "            return category\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2b4f7",
   "metadata": {},
   "source": [
    "### Text Embedding and Summarization Functions\n",
    "\n",
    "Set up functions for generating combined text embeddings, extractive and abstractive summarization, and text highlighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e06026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a combined embedding for a given text\n",
    "def get_combined_embedding(text):\n",
    "    # Tokenize the text and generate embeddings\n",
    "    words = word_tokenize(text)\n",
    "    word_embeddings = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    w2v_embedding = np.mean(word_embeddings, axis=0) if word_embeddings else np.zeros(word2vec_model.vector_size)\n",
    "    tfidf_embedding = tfidf_vectorizer.transform([text]).toarray()[0]\n",
    "    combined_embedding = np.hstack((w2v_embedding, tfidf_embedding))\n",
    "    return combined_embedding\n",
    "\n",
    "# Extractive summarization function using spaCy\n",
    "def summarize_text_spacy(text, num_sentences=3):\n",
    "    doc = nlp(text)\n",
    "    word_frequencies = Counter(token.text.lower() for token in doc if not token.is_stop and not token.is_punct)\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    sentence_scores = {sentence: sum(word_frequencies[token.text.lower()] for token in sentence) / max_frequency for sentence in doc.sents}\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    return ' '.join([sentence.text for sentence in summary_sentences])\n",
    "\n",
    "# Abstractive summarization function using Hugging Face Transformers\n",
    "def generate_summary(text):\n",
    "    return abstractive_summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "\n",
    "# Summarization function using summa\n",
    "def summarize_text_summa(text, num_keywords=10):\n",
    "    return keywords.keywords(text, words=num_keywords)\n",
    "\n",
    "# Function to highlight context in the text\n",
    "def highlight_context(full_text, user_input):\n",
    "    # Split the full text into sentences\n",
    "    sentences = full_text.split('.')\n",
    "    \n",
    "    # Identify keywords in the user's query\n",
    "    keywords = user_input.split()\n",
    "    \n",
    "    # Initialize list to store highlighted sentences\n",
    "    highlighted_sentences = []\n",
    "    \n",
    "    # Iterate through sentences to find and highlight relevant portions\n",
    "    for sentence in sentences:\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in sentence.lower():\n",
    "                # Highlight the keyword in the sentence\n",
    "                highlighted_sentence = sentence.replace(keyword, f'**{keyword}**')\n",
    "                highlighted_sentences.append(highlighted_sentence)\n",
    "                break\n",
    "    \n",
    "    # Combine highlighted sentences into a single text\n",
    "    highlighted_text = '. '.join(highlighted_sentences)\n",
    "    \n",
    "    return highlighted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e02527",
   "metadata": {},
   "source": [
    "### Chatbot Response Generation\n",
    "\n",
    "Create a function to generate chatbot responses based on user inputs, using the most relevant document and summarization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most relevant document\n",
    "def find_most_relevant_document(input_text, filtered_df):\n",
    "    input_embedding = get_combined_embedding(input_text)\n",
    "    max_similarity = 0\n",
    "    most_similar_doc_index = None\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        doc_embedding = combined_embeddings[index]\n",
    "        similarity = cosine_similarity([input_embedding], [doc_embedding])[0][0]\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_similar_doc_index = index\n",
    "    return most_similar_doc_index, max_similarity\n",
    "\n",
    "# Function to generate chatbot response\n",
    "def generate_chatbot_response(user_input):\n",
    "    user_category = determine_category(user_input)\n",
    "    filtered_df = df[df['Category'] == user_category]\n",
    "    doc_index, similarity = find_most_relevant_document(user_input, filtered_df)\n",
    "    \n",
    "    if similarity > 0.15 and doc_index is not None:\n",
    "        full_text = df.iloc[doc_index]['Cleaned Text']\n",
    "        highlighted_text = highlight_context(full_text, user_input)  # Function to highlight relevant portions\n",
    "        if highlighted_text.strip():\n",
    "            response = f\"Here is the information I found:\\n\\n{highlighted_text}\"\n",
    "        else:\n",
    "            response = \"I found a document, but it doesn't contain enough information to summarize. Please try a more specific query or consult the LogRhythm documentation.\"\n",
    "    else:\n",
    "        response = \"I'm sorry, I couldn't find relevant information based on your query. Please try asking differently or consult the LogRhythm documentation.\"\n",
    "    \n",
    "    return response    \n",
    "\n",
    "# Main chatbot interaction loop\n",
    "print(\"LogRhythm Chatbot â€” Type 'quit' to exit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"\\033[1mLogRhythm Chatbot: Goodbye!\\033[0m\")\n",
    "        break\n",
    "    response = generate_chatbot_response(user_input)\n",
    "    print(\"\\033[1mLogRhythm Chatbot:\\033[0m\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
